---
title: Overview
description: OLake getting started overview
sidebar_position: 1
---

# Overview

This getting started guide provides a comprehensive overview of OLake, for you to get started with OLake. 

## Where you can run OLake:

Below are different ways you can run OLake. 

| Platform / Orchestration | Quick-start or setup docs | Comments |
| :---- | :---- | :---- |
| OLake UI | [LINK](https://github.com/datazip-inc/olake/blob/master/README.md) | (Recommended)|
| Local system (bare metal / laptop) | [Local Iceberg Setup (with minIO)](/docs/writers/iceberg/docker-compose) |  |
| Stand-alone **Docker** container | [using OLake docker image](/docs/getting-started/postgres) | Easiest for PoC; ships the CLI plus driver in one image. |
| **Airflow on EC2** | [EC2 DAG how-to](https://olake.io/blog/olake-airflow-on-ec2) | Spins up a short-lived EC2 worker that pulls the OLake image, runs `sync`, then terminates. |
| **Airflow on Kubernetes** | [K8s + Airflow example](https://olake.io/blog/olake-airflow) ([OLake](https://olake.io/blog/olake-airflow)) | Same DAG, but the `KubernetesPodOperator` schedules OLake pods inside the cluster. |


## Connectors
OLake supports a variety of data sources and destinations. Below is a list of currently supported connectors:

| | Sources | Links | 
| ---| --------| ----- |
| <img src="/img/logo/mongodb.webp"  className="mt-3"  height={20} /> |  MongoDB  | [Docs](../connectors/mongodb/overview) / [Code](https://github.com/datazip-inc/olake/tree/master/drivers/mongodb) / [Getting Started](../getting-started/mongodb) |
| <img src="/img/logo/postgres.png"  className="mt-3"  height={20} /> | Postgres | [Docs](../connectors/postgres/overview) / [Code](https://github.com/datazip-inc/olake/tree/master/drivers/postgres) / [Getting Started](../getting-started/postgres) |
| <img src="/img/logo/mysql.png"  className="mt-3"  height={20} /> | MySQL |   [Docs](../connectors/mysql/overview) / [Code](https://github.com/datazip-inc/olake/tree/master/drivers/mysql) / [Getting Started](../getting-started/mysql) | 
| <img src="/img/logo/s3.png"  className="mt-3"  height={20} /> | AWS S3 |   [check issue](https://github.com/datazip-inc/olake/issues/86) | 
| <img src="/img/logo/kafka.png"  className="mt-3"  height={20} /> | Kafka |    [check issue](https://github.com/datazip-inc/olake/issues/87) | 


## Destinations

| Destination       | Supported | Docs | Comments |
|-------------------|-----------| ---- | ------- |
| <img src="/img/logo/iceberg.png"  className="mt-3"  height={20} /> Apache Iceberg           | Yes       | [Link](../writers/iceberg/overview) | |
| <img src="/img/logo/s3.png"  className="mt-3"  height={20} /> AWS S3                | Yes       | [Link](../writers/parquet/s3/) |  Supports both plain-Parquet and Iceberg layouts; requires `aws_access_key` / IAM role. |
| <img src="/img/logo/azure-adls.png"  className="mt-3"  height={20} /> Azure                 | Yes       | [Link](../writers/iceberg/azure/) | |
| <img src="/img/logo/gcs.png"  className="mt-3"  height={20} /> Google Cloud Storage                | Yes       | [Link](../writers/iceberg/gcs/) | Any S3 protocol compliant object store can work with OLake |
| <img src="/img/logo/local.png"  className="mt-3"  height={20} /> Local Filesystem  | Yes       | [Link](../writers/parquet/local) | |


## File formats OLake can emit

| Output format | Docs | Comments |
| :---- | :---- | ----- |
| **Apache Iceberg** tables | [Iceberg writer overview](/docs/writers/iceberg/catalog/overview) | Full snapshot + incremental CDC → Iceberg; works with all catalogs listed below. |
| **Parquet** files | [Parquet writer modes](/docs/writers/parquet/config) | Simple columnar dumps (no table metadata); choose local or S3 sub-mode. |


## Query OLake dumped data Using Different Catalogs

1. AWS Glue + AWS Athena  
2. AWS Glue + Spark  
3. AWS Glue + Snowflake  
4. AWS Glue + DuckDB  
5. REST Catalog + DuckDB  
6. REST catalog + ClickHouse  



## Iceberg catalogs OLake can register to

| Catalog type | Docs / example | Comments |
| :---- | ----- | ----- |
| REST – **Lakekeeper** | [LINK](/docs/writers/iceberg/catalog/rest#using-lakekeeper) | [Officially Supported] Rust-native catalog with optimistic locking; Helm chart available for K8s. |
| REST – **Gravitino** | [LINK](/docs/writers/iceberg/catalog/rest#using-gravitino) | [Yet to be tested] Uses standard Iceberg REST; Gravitino adds multi-cloud routing. |
| REST – **Nessie** | [LINK](/docs/writers/iceberg/catalog/rest#using-nessie) | [Yet to be tested] Time-travel branches/tags; supply `nessie.endpoint` in `destination.json`. |
| REST – **Polaris** | [LINK](/docs/writers/iceberg/catalog/rest#using-polaris) | [Yet to be tested]  |
| **Hive Metastore** | [Hive catalog config](/docs/writers/iceberg/catalog/hive) | Classic HMS; good fit for on-prem Hadoop or EMR. |
| **JDBC Catalog** (Postgres/MySQL) | [JDBC catalog sample](/docs/writers/iceberg/catalog/jdbc) | Stores Iceberg metadata in an RDBMS; easiest to spin up locally with Postgres. |
| **AWS Glue Catalog** | [Glue catalog IAM & config](/docs/writers/iceberg/catalog/glue) | Best choice on AWS; lets Athena, EMR, and Redshift query the same tables instantly. |

## Data from OLake on Apache Iceberg can be queried from:


| QUERY TOOL  | AWS Glue Catalog | Hive Metastore  | JDBC Catalog | Iceberg REST Catalog |
| ----- | ----- |  :---: | :---: | :---: |
| Amazon Athena |   ✅ | ❌ | ❌ | ❌ |
| Apache Spark (v3.3 +) |   ✅ | ✅ | ✅ | ✅ |
| Apache Flink (v1.18 +) |   ✅ | ✅ | ✅ | ✅ |
| Trino (v475 +) |   ✅ | ✅ | ✅ | ✅ |
| Starburst Enterprise |   ✅ | ✅ | ✅ | ✅ |
| Presto (v0.288 +) |   ✅ | ✅ | ✅ | ✅ |
| Apache Hive v4.0 |   ✅ | ✅ | ❌ | ✅ |
| Apache Impala v4.4   | ❌ | ✅ | ❌ | ❌ |
| Dremio v25/26 |   ✅ | ✅ | ❌ | ✅ |
| DuckDB v1.2.1 |   ❌ | ❌ | ❌ | ✅ |
| ClickHouse v24.3 +   | ❌ | ❌ | ❌ | ✅ |
| StarRocks v3.2 + |   ❌ | ✅ | ❌ | ✅ |
| Apache Doris v2.1 +   | ✅ | ✅ | ❌ | ✅ |
| Google BigQuery (BigLake)   | ❌ | ❌ | ❌ | ❌* |
| Snowflake (Iceberg GA)   | ❌ | ❌ | ❌ | ✅ |
| Databricks (Unity Catalog API)   | ❌ | ❌ | ❌ | ✅ |


**BigQuery’s BigLake tables read Iceberg manifests directly without using an Iceberg catalog, so none of the four catalog types apply.*


| Sl | Query / Analytics Engine (“tool”) |  Link | Supported Iceberg catalogs | Comments |
| ----- | ----- | ----- | :------------- | ----- |
| 1 | Amazon Athena | [Query Iceberg tables] [AWS Documentation](https://docs.aws.amazon.com/athena/latest/ug/querying-iceberg.html) | Only the **AWS Glue Data Catalog**  | Athena-v3 can read & write Iceberg v2 tables **only** when they’re registered in Glue. |
| 2 | Apache Spark (3.3 → 4.x) | [Spark catalog config] [Apache Iceberg](https://iceberg.apache.org/docs/latest/spark-configuration/) | Hive Metastore, Hadoop warehouse, REST, AWS Glue, JDBC, Nessie, plus custom plug-ins via `spark.sql.catalog.*` settings. | Configure with `spark.sql.catalog.<name>.type` = `glue |
| 3 | Apache Flink (1.18+) | [Flink catalog options] [Apache Iceberg](https://iceberg.apache.org/docs/1.8.0/flink-configuration/) | Hive Metastore, Hadoop catalog, REST catalog (incl. Nessie), AWS Glue, JDBC, plus any custom implementation via `catalog-impl`.  | Set catalog-type=glue |
| 4 | Trino ( ≥ 475 ) | [Iceberg metastores] [trino.io](https://trino.io/docs/current/object-storage/metastores.html) | Hive Metastore, AWS Glue, JDBC, REST, Nessie, or Snowflake; | iceberg.catalog.type=glue |
| 5 | Starburst Enterprise (SEP 413 →) | [SEP Iceberg connector] [Starburst](https://docs.starburst.io/latest/object-storage/metastores.html) | Hive Metastore, AWS Glue, JDBC, REST, Nessie, Snowflake, and Starburst Galaxy’s managed metastore | Same config keys as Trino; extra features (Warp Speed, Polaris). |
| 6 | PrestoDB (0.288+) | [Lab guide using REST] [IBM GitHub](https://ibm.github.io/presto-iceberg-lab/lab-1/) | Hive Metastore, AWS Glue, REST/Nessie (0.277+ with OAuth 2), Hadoop (file-based); JDBC | Glue needs the AWS SDK fat-jar; REST landed in 0.288. |
| 7 | Apache Hive (4.0) | [Hive integration] [Apache Iceberg](https://iceberg.apache.org/docs/latest/hive/) | Hive Metastore is the default catalog; Hadoop, REST/Nessie, AWS Glue, JDBC, or custom catalogs can also be configured. | Uses StorageHandler; for Glue add AWS bundle. |
| 8 | Apache Impala (4.4) | [Impala Iceberg docs] [Impala](https://impala.apache.org/docs/build/html/topics/impala_iceberg.html) | Hive Metastore, HadoopCatalog, HadoopTables, Other Iceberg catalog-impl values can be registered in *Hive* site-config (but Impala itself cannot talk to Glue/REST/Nessie directly). | Can create & query Iceberg; Glue via HMS federation only. |
| 9 | Dremio (25/26) | [REST catalog support] [Dremio Documentation](https://docs.dremio.com/current/release-notes/version-260-release/) | Polaris / “Dremio Catalog” (built-in REST, Nessie-backed)** • **Generic Iceberg REST Catalog** (Tabular, Unity, Snowflake Open, etc.) • Arctic (Nessie) sources via UI • Hive Metastore (HMS) • AWS Glue • Hadoop (file-based) • **Nessie stand-alone** endpoint supported as Source | Add source in **Lakehouse Catalogs** UI; JDBC not yet. |
| 10 | DuckDB (1.2.1) | [Iceberg extension] [DuckDB](https://duckdb.org/docs/stable/extensions/iceberg/overview.html) [REST preview] [DuckDB](https://duckdb.org/2025/03/14/preview-amazon-s3-tables.html) | • **Hadoop (File-system)** — still the simplest path: `iceberg_scan('/bucket/table/')` or a direct metadata JSON. • **Iceberg REST catalogs** (e.g., Nessie, Tabular) supported via the `rest` option; now accepts bearer/OAuth tokens through the new `rest_auth_token` parameter. • No native Hive/Glue catalog yet, but you can proxy them through REST. | `INSTALL iceberg; LOAD iceberg;` then `ATTACH 'nessie://…'` or `ATTACH 'glue+rest://…'`. |
| 11 | ClickHouse (24.3+) | [Iceberg table engine] [ClickHouse](https://clickhouse.com/docs/engines/table-engines/integrations/iceberg) | *Path (Hadoop-style)* since 24.3, *REST catalog* (Nessie, Polaris/Unity, Glue REST) in 24.12 via `SETTINGS catalog_type='rest'`; *Hive Metastore experimental* & *AWS Glue* integrations in testing;R2 (Cloudflare) REST catalog **on roadmap**. | Read-only engine; REST integration roadmap notes catalog auto-discovery. |
| 12 | StarRocks (3.2+) | [StarRocks Iceberg quick-start][StarRocks Documentation](https://docs.starrocks.io/docs/quick_start/iceberg/) | Hive Metastore, AWS Glue Data Catalog (requires S3 storage), REST, (default, with AWS Glue REST endpoint support), JDBC-compatible metastore | Create `CREATE CATALOG iceberg … PROPERTIES('type'='iceberg','metastore'='…')`. |
| 13 | Apache Doris (2.1+) | [Doris Iceberg catalog] [doris.apache.org](https://doris.apache.org/docs/dev/lakehouse/best-practices/doris-iceberg) | Hive Metastore, REST, Hadoop (filesystem metadata), AWS Glue, Alibaba Cloud DLF, AWS S3 Tables Catalog. | Multi-catalog; external queries and **writes** supported since v3.1 |
| 14 | Google BigQuery (BigLake) | [BigLake Iceberg tables] [Google Cloud](https://cloud.google.com/bigquery/docs/iceberg-external-tables) | You **cannot configure BigQuery to use REST, Hive, or Glue catalogs for writing** Iceberg tables. | Creates **external tables**; catalog-less — points directly at manifests in GCS/S3. |
| 15 | Snowflake (2025 GA) | [Snowflake Iceberg tables] [Snowflake Documentation](https://docs.snowflake.com/en/user-guide/tables-iceberg) | Snowflake-native, REST (read-only); **External-catalog Iceberg tables** (whether via AWS Glue, Iceberg metadata files, Open Catalog/Polaris, or remote REST catalogs) are **read-only in Snowflake** | Snowflake maintains its own catalog but can **read** external REST catalogs (Unity, Glue REST). |
| 16 | Databricks Unity Catalog | [UC Iceberg REST endpoint] [Databricks Documentation](https://docs.databricks.com/gcp/en/external-access/iceberg)[Databricks Documentation](https://docs.databricks.com/aws/en/external-access/integrations) | Full support for the Iceberg REST Catalog API, allowing external engines to read (Generally Available) and write (Public Preview) to Unity Catalog–managed Iceberg tables. Iceberg catalog federation is in Public Preview, enabling you to govern and query Iceberg tables managed in AWS Glue, Hive Metastore, and Snowflake Horizon without copying data. | Endpoint `/api/2.1/unity-catalog/iceberg`; external clients Spark, Flink, Trino, DuckDB, ClickHouse, etc. |
| 17 | e6data Lakehouse Compute Engine | [e6data × S3 Tables] [e6data.com](https://www.e6data.com/blog/e6data-integrates-with-s3-tables) | AWS Glue, REST, Hive (read) | Serverless SQL engine; advertises compatibility with “all table formats & catalogs.” |

* Glue support in Presto requires the `hive.metastore=glue` shim or running Presto inside AWS EMR.

**Notes & gotchas**

* **REST flavours.** “REST” above covers standard Iceberg REST plus branded servers (Nessie, Lakekeeper, Gravitino, AWS Glue REST endpoint, Databricks Unity, Snowflake Polaris).  
* **JDBC catalog** is production-ready in Spark, Flink, Trino/Starburst, and Presto. Engines not listed in that column (e.g., ClickHouse) cannot yet use it.  
* **Hive vs Hadoop.** Some engines list “Hadoop Catalog” separately (path-based, no service). I’ve rolled those under *Hive* here if the engine simply re-uses the HMS client.  
* **Read-only vs read-write.** ClickHouse and BigQuery are read-only; Athena supports `INSERT/UPDATE/MERGE` (MoR only); most others are full read-write when using Glue/Hive/JDBC/REST.

