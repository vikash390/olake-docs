---
title: Apache Flink 1.18+
description: The reference implementation for CDC to Iceberg with comprehensive streaming support, exactly-once semantics, and advanced incremental reads
hide_table_of_contents: true
---

import { QueryEngineLayout } from '@site/src/components/Iceberg/QueryEngineLayout';
import { 
  ServerStackIcon,
  // DatabaseIcon,
  BoltIcon,
  ShieldCheckIcon,
  ClockIcon,
  CodeBracketIcon,
  CubeIcon,
  CloudIcon
} from '@heroicons/react/24/outline';

export const flinkFeatures = [
  {
    title: "Comprehensive Catalog Support",
    chip: "Full Support",
    description: "Hive Metastore, Hadoop catalog, REST catalog (incl. Nessie), AWS Glue, JDBC, plus any custom implementation via catalog-impl",
    icon: <BoltIcon className="w-6 h-6" />,
    color: "blue",
    score: 100,
    details: {
      title: "Multi-Catalog Integration in Flink",
      description: "Flink provides comprehensive catalog support enabling seamless integration with existing metadata infrastructure and modern cloud-native solutions.",
      overviewContent: {
        strengths: [
          "Support for all major catalog implementations",
          "Dynamic catalog registration with CREATE CATALOG statements",
          "Custom catalog implementations via catalog-impl property",
          "Seamless integration with existing Hadoop ecosystems",
          "Native cloud catalog support (AWS Glue, Nessie)"
        ],
        limitations: [
          "Catalog-specific configuration required for each type",
          "Some advanced catalog features may vary by implementation",
          "Cross-catalog operations not supported in single queries"
        ],
        bestFor: [
          "Organizations with existing Hive Metastore infrastructure",
          "Multi-cloud deployments requiring catalog flexibility",
          "Teams migrating from Hadoop to cloud-native solutions",
          "Environments requiring custom metadata management"
        ]
      },
      technicalSpecs: [
        {
          category: "Supported Catalog Types",
          items: [
            { label: "Hive Metastore", value: "Full Support", status: "available" },
            { label: "Hadoop Catalog", value: "Full Support", status: "available" },
            { label: "REST Catalog", value: "Including Nessie", status: "available" },
            { label: "AWS Glue", value: "Full Support", status: "available" },
            { label: "JDBC Catalog", value: "Full Support", status: "available" },
            { label: "Custom Implementation", value: "Via catalog-impl", status: "available" }
          ]
        },
        {
          category: "Configuration Features",
          items: [
            { label: "Dynamic Registration", value: "CREATE CATALOG", status: "available" },
            { label: "Multi-Catalog Support", value: "Yes", status: "available" },
            { label: "Catalog Properties", value: "Flexible", status: "available" },
            { label: "Security Integration", value: "Catalog-dependent", status: "available" }
          ]
        }
      ],
      externalLinks: [
        { label: "Flink Catalog Configuration", url: "https://iceberg.apache.org/docs/latest/flink-configuration/#catalog-configuration", type: "docs" }
      ]
    }
  },
  {
    title: "Streaming & CDC Excellence",
    chip: "Reference Engine",
    description: "Reference engine for CDC → Iceberg: consume Debezium/Kafka changelogs, upsert with exactly-once semantics, FLIP-27 incremental reads",
    icon: <BoltIcon className="w-6 h-6" />,
    color: "green",
    score: 100,
    details: {
      title: "CDC and Streaming Leadership",
      description: "Flink serves as the reference implementation for Change Data Capture workflows with Iceberg, providing industry-leading streaming capabilities.",
      overviewContent: {
        strengths: [
          "Exactly-once semantics for streaming writes",
          "Native Debezium and Kafka CDC integration",
          "FLIP-27 source for unified batch and streaming reads",
          "Automatic checkpoint-based commits to Iceberg",
          "Advanced incremental processing strategies"
        ],
        limitations: [
          "FLIP-27 source still skips deletion vector snapshots",
          "Streaming MERGE operations require long-running jobs",
          "CDC read support limited in some scenarios"
        ],
        bestFor: [
          "Real-time CDC pipelines from databases to data lakes",
          "Event streaming architectures with Kafka",
          "Near real-time analytics requiring low latency",
          "Complex event processing with state management"
        ]
      },
      technicalSpecs: [
        {
          category: "CDC Capabilities",
          items: [
            { label: "Debezium Integration", value: "Native Support", status: "available" },
            { label: "Kafka CDC", value: "Full Support", status: "available" },
            { label: "Exactly-Once Semantics", value: "Yes", status: "available" },
            { label: "Multi-Source CDC", value: "Yes", status: "available" }
          ]
        },
        {
          category: "Streaming Features",
          items: [
            { label: "FLIP-27 Source", value: "Incremental Reads", status: "available" },
            { label: "Checkpoint Commits", value: "Automatic", status: "available" },
            { label: "Watermark Support", value: "Yes", status: "available" },
            { label: "State Management", value: "Built-in", status: "available" }
          ]
        }
      ],
      externalLinks: [
        { label: "Apache Flink CDC Overview", url: "https://nightlies.apache.org/flink/flink-cdc-docs-master/docs/connectors/flink-sources/overview/", type: "docs" },
        { label: "CDC with Apache Iceberg", url: "https://www.dremio.com/blog/cdc-with-apache-iceberg/", type: "blog" },
        { label: "Debezium Iceberg Integration", url: "https://debezium.io/blog/2021/10/20/using-debezium-create-data-lake-with-apache-iceberg/", type: "blog" },
        { label: "Iceberg Pipeline Connector", url: "https://nightlies.apache.org/flink/flink-cdc-docs-master/docs/connectors/pipeline-connectors/iceberg/", type: "docs" }
      ]
    }
  },
  {
    title: "Batch and Real-time Processing",
    chip: "Full Support",
    description: "Batch and streaming jobs read snapshots or incremental DataStreams; Iceberg Sink commits on each checkpoint with exactly-once semantics",
    icon: <ServerStackIcon className="w-6 h-6" />,
    color: "purple",
    score: 100,
    details: {
      title: "Unified Batch and Streaming Architecture",
      description: "Flink provides a unified processing model that seamlessly handles both batch and streaming workloads with Iceberg tables.",
      overviewContent: {
        strengths: [
          "Unified API for batch and streaming processing",
          "Snapshot-based batch reads with optimization",
          "Incremental streaming reads with configurable strategies",
          "Exactly-once processing guarantees",
          "Automatic checkpoint-based commits"
        ],
        limitations: [
          "Batch INSERT OVERWRITE vs streaming append differences",
          "Some streaming features not available in batch mode",
          "Performance characteristics vary between modes"
        ],
        bestFor: [
          "Lambda architectures requiring unified processing logic",
          "Organizations transitioning from batch to streaming",
          "Mixed workloads with both historical and real-time processing",
          "Applications requiring consistent processing semantics"
        ]
      },
      technicalSpecs: [
        {
          category: "Batch Processing",
          items: [
            { label: "Snapshot Reads", value: "Optimized", status: "available" },
            { label: "INSERT OVERWRITE", value: "Supported", status: "available" },
            { label: "Table Scans", value: "Full Support", status: "available" },
            { label: "Metadata Operations", value: "Yes", status: "available" }
          ]
        },
        {
          category: "Streaming Processing",
          items: [
            { label: "Incremental Reads", value: "FLIP-27 Source", status: "available" },
            { label: "Exactly-Once Writes", value: "Checkpoint-based", status: "available" },
            { label: "Starting Strategies", value: "Configurable", status: "available" },
            { label: "Watermark Processing", value: "Yes", status: "available" }
          ]
        }
      ],
      externalLinks: [
        { label: "Flink Queries - Batch & Streaming", url: "https://iceberg.apache.org/docs/latest/flink-queries/", type: "docs" },
        { label: "Flink Writes", url: "https://iceberg.apache.org/docs/latest/flink-writes/", type: "docs" },
        { label: "Incremental Read Strategies", url: "https://iceberg.apache.org/docs/1.4.3/flink-configuration/#starting-strategy", type: "docs" }
      ]
    }
  },
  {
    title: "UPSERT and Row-level Operations",
    chip: "Partial Support",
    description: "INSERT append always available; row-level changes via write.upsert.enabled=true on spec v2 tables; MERGE INTO not supported in Flink SQL",
    icon: <CodeBracketIcon className="w-6 h-6" />,
    color: "orange",
    score: 75,
    details: {
      title: "Row-level Data Modification",
      description: "Flink provides row-level modification capabilities through UPSERT mode, though with some SQL limitations compared to other engines.",
      overviewContent: {
        strengths: [
          "UPSERT mode for CDC and streaming scenarios",
          "Automatic primary key deduplication",
          "Efficient delete file generation for MoR",
          "Integration with streaming CDC workflows",
          "Exactly-once upsert semantics"
        ],
        limitations: [
          "MERGE INTO not supported in Flink SQL",
          "Requires write.upsert.enabled=true configuration",
          "Limited to spec v2 tables for row-level operations",
          "Ad-hoc row-level SQL operations are limited"
        ],
        bestFor: [
          "CDC pipelines requiring upsert semantics",
          "Streaming applications with duplicate handling",
          "Real-time data synchronization scenarios",
          "Applications with primary key-based updates"
        ]
      },
      technicalSpecs: [
        {
          category: "DML Operations",
          items: [
            { label: "INSERT INTO", value: "Full Support", status: "available" },
            { label: "INSERT OVERWRITE", value: "Batch Only", status: "available" },
            { label: "UPSERT Mode", value: "Via Configuration", status: "available" },
            { label: "MERGE INTO", value: "Not Supported", status: "limited" }
          ]
        },
        {
          category: "UPSERT Features",
          items: [
            { label: "Primary Key Dedup", value: "Automatic", status: "available" },
            { label: "Delete File Generation", value: "MoR Mode", status: "available" },
            { label: "Streaming Upserts", value: "Yes", status: "available" },
            { label: "Spec v2 Requirement", value: "Yes", status: "available" }
          ]
        }
      ],
      externalLinks: [
        { label: "Flink INSERT INTO", url: "https://iceberg.apache.org/docs/latest/flink-writes/#insert-into", type: "docs" },
        { label: "UPSERT Mode", url: "https://iceberg.apache.org/docs/latest/flink-writes/#upsert", type: "docs" },
        { label: "Delete Data Feature", url: "https://docs.cloudera.com/cdw-runtime/1.5.5/iceberg-how-to/topics/iceberg-delete.html", type: "docs" }
      ]
    }
  },
  {
    title: "MoR/CoW Storage Strategies",
    chip: "Full Support", 
    description: "Copy-on-Write for static batch rewrites; Merge-on-Read for streaming/UPSERT with delete files instead of partition rewrites",
    icon: <CubeIcon className="w-6 h-6" />,
    color: "blue",
    score: 100,
    details: {
      title: "Adaptive Storage Strategies",
      description: "Flink automatically selects optimal storage strategies based on workload patterns, providing efficient Copy-on-Write and Merge-on-Read operations.",
      overviewContent: {
        strengths: [
          "Automatic strategy selection based on operation type",
          "Copy-on-Write for batch rewrites and optimal read performance",
          "Merge-on-Read for streaming and frequent updates",
          "Delete file generation instead of partition rewrites",
          "Optimized for both read and write performance"
        ],
        limitations: [
          "Strategy choice depends on operation mode (batch vs streaming)",
          "MoR requires understanding of read-time merge overhead",
          "CoW operations have higher write latency"
        ],
        bestFor: [
          "Mixed workloads with varying access patterns",
          "Streaming applications with frequent updates",
          "Batch processing requiring optimal read performance",
          "CDC scenarios with high update frequency"
        ]
      },
      technicalSpecs: [
        {
          category: "Copy-on-Write",
          items: [
            { label: "Batch INSERT OVERWRITE", value: "Optimal", status: "available" },
            { label: "Read Performance", value: "Excellent", status: "available" },
            { label: "Write Latency", value: "Higher", status: "available" },
            { label: "Storage Efficiency", value: "High", status: "available" }
          ]
        },
        {
          category: "Merge-on-Read",
          items: [
            { label: "Streaming UPSERT", value: "Optimal", status: "available" },
            { label: "Delete Files", value: "Instead of Rewrites", status: "available" },
            { label: "Write Performance", value: "Excellent", status: "available" },
            { label: "Read Overhead", value: "Merge Required", status: "available" }
          ]
        }
      ],
      externalLinks: [
        { label: "Flink Batch INSERT OVERWRITE", url: "https://iceberg.apache.org/docs/latest/flink/#writing-with-sql", type: "docs" },
        { label: "UPSERT Mode MoR", url: "https://iceberg.apache.org/docs/1.5.0/flink-writes/#upsert", type: "docs" }
      ]
    }
  },
  {
    title: "Format V3 Support",
    chip: "GA",
    description: "GA read + write with Flink 1.18+ and Iceberg 1.8+; Binary Deletion Vectors, Row Lineage, new data types, multi-argument transforms",
    icon: <CloudIcon className="w-6 h-6" />,
    color: "purple",
    score: 95,
    details: {
      title: "Advanced Format V3 Capabilities",
      description: "Flink provides comprehensive Format V3 support with advanced features like deletion vectors and row lineage tracking.",
      overviewContent: {
        strengths: [
          "GA read and write support with proper versions",
          "Binary deletion vectors for efficient deletes",
          "Automatic row lineage tracking",
          "New data types including variant and geospatial",
          "Multi-argument partition transforms",
          "True MoR semantics with deletion vectors"
        ],
        limitations: [
          "FLIP-27 incremental source still skips DV snapshots",
          "Lineage and DV converge fully only in Iceberg 1.9+",
          "Geospatial types are read-only currently",
          "Flink ≤ 1.17 cannot author DV/lineage"
        ],
        bestFor: [
          "Modern data platforms requiring latest capabilities",
          "High-frequency update workloads benefiting from DVs",
          "Applications requiring detailed audit trails",
          "Advanced analytics with lineage tracking"
        ]
      },
      technicalSpecs: [
        {
          category: "Format V3 Features",
          items: [
            { label: "Binary Deletion Vectors", value: "GA Support", status: "available" },
            { label: "Row Lineage", value: "_row_id, _last_updated_sequence_number", status: "available" },
            { label: "New Data Types", value: "variant, timestamp_ns", status: "available" },
            { label: "Geospatial Types", value: "geometry, geography (read-only)", status: "preview" },
            { label: "Multi-arg Transforms", value: "bucket[4](user_id, ds)", status: "available" }
          ]
        },
        {
          category: "Version Requirements",
          items: [
            { label: "Flink 1.18+", value: "Full V3 read/write", status: "available" },
            { label: "Iceberg 1.8+", value: "Required for GA", status: "available" },
            { label: "Runtime JAR", value: "iceberg-flink-runtime-1.18", status: "available" },
            { label: "FLIP-27 DV Support", value: "Limited", status: "preview" }
          ]
        }
      ],
      externalLinks: [
        { label: "Multi-Engine Support", url: "https://iceberg.apache.org/multi-engine-support/", type: "docs" },
        { label: "Format V3 Specification", url: "https://iceberg.apache.org/spec/#version-3-extended-types-and-capabilities", type: "docs" },
        { label: "Iceberg 1.8.0 Release", url: "https://iceberg.apache.org/releases/#1.8.0-release", type: "docs" },
        { label: "New Data Types", url: "https://www.dremio.com/blog/apache-iceberg-v3/", type: "blog" }
      ]
    }
  },
  {
    title: "Time Travel & Incremental Reads",
    chip: "Full Support",
    description: "Filter push-down + partition pruning automatic; point-in-time reads via source options: start-snapshot-id, start-snapshot-timestamp, branch, tag",
    icon: <ClockIcon className="w-6 h-6" />,
    color: "orange",
    score: 100,
    details: {
      title: "Advanced Time Travel and Incremental Processing",
      description: "Flink provides sophisticated time travel capabilities and incremental read strategies optimized for streaming and batch workloads.",
      overviewContent: {
        strengths: [
          "Flexible point-in-time read options",
          "Advanced incremental read strategies",
          "Automatic filter pushdown and partition pruning",
          "Branch and tag support for data versioning",
          "FLIP-27 source for unified incremental processing"
        ],
        limitations: [
          "Time travel options vary between batch and streaming modes",
          "Some incremental strategies may skip certain snapshot types",
          "Performance depends on snapshot retention policies"
        ],
        bestFor: [
          "Incremental ETL processing with checkpointing",
          "Data validation and quality checks",
          "Audit and compliance scenarios",
          "Streaming applications requiring historical context"
        ]
      },
      technicalSpecs: [
        {
          category: "Time Travel Options",
          items: [
            { label: "start-snapshot-id", value: "Specific Snapshot", status: "available" },
            { label: "start-snapshot-timestamp", value: "Point-in-time", status: "available" },
            { label: "branch", value: "Branch-based Reads", status: "available" },
            { label: "tag", value: "Tag-based Reads", status: "available" }
          ]
        },
        {
          category: "Incremental Strategies",
          items: [
            { label: "TABLE_SCAN_THEN_INCREMENTAL", value: "Full then incremental", status: "available" },
            { label: "INCREMENTAL_FROM_LATEST_SNAPSHOT", value: "Latest snapshot forward", status: "available" },
            { label: "Filter Pushdown", value: "Automatic", status: "available" },
            { label: "Partition Pruning", value: "Automatic", status: "available" }
          ]
        }
      ],
      externalLinks: [
        { label: "Flink Read Options", url: "https://iceberg.apache.org/docs/latest/flink-configuration/#read-options", type: "docs" },
        { label: "Performance Data Filtering", url: "https://iceberg.apache.org/docs/latest/performance/#data-filtering", type: "docs" }
      ]
    }
  },
  {
    title: "Enterprise Security",
    chip: "Delegated",
    description: "Inherits ACLs from underlying catalog (Hive Ranger, AWS IAM, Nessie authorization); REST catalog secured with credential/token properties",
    icon: <ShieldCheckIcon className="w-6 h-6" />,
    color: "green",
    score: 100,
    details: {
      title: "Comprehensive Security Integration",
      description: "Flink integrates with enterprise security frameworks through catalog delegation and supports modern authentication mechanisms.",
      overviewContent: {
        strengths: [
          "Integration with existing enterprise security systems",
          "Multi-tenant REST catalog authentication",
          "Fine-grained access control through catalogs",
          "OAuth2 and token-based authentication support",
          "S3 access grants and IAM integration"
        ],
        limitations: [
          "Security features depend on catalog implementation",
          "Flink enforces only connector-level permissions",
          "Cross-catalog security policies not unified"
        ],
        bestFor: [
          "Enterprise environments with existing security infrastructure",
          "Multi-tenant deployments requiring isolation",
          "Cloud-native applications with modern auth",
          "Organizations with complex compliance requirements"
        ]
      },
      technicalSpecs: [
        {
          category: "Access Control Systems",
          items: [
            { label: "Apache Ranger", value: "Hive integration", status: "available" },
            { label: "AWS IAM", value: "S3 + Glue permissions", status: "available" },
            { label: "Nessie RBAC", value: "Branch-level policies", status: "available" },
            { label: "REST Catalog Auth", value: "OAuth2, tokens", status: "available" }
          ]
        },
        {
          category: "Authentication Methods",
          items: [
            { label: "Credential Properties", value: "REST catalogs", status: "available" },
            { label: "Token Authentication", value: "Bearer tokens", status: "available" },
            { label: "S3 Access Grants", value: "IAM integration", status: "available" },
            { label: "HDFS Authentication", value: "Kerberos", status: "available" }
          ]
        }
      ],
      externalLinks: [
        { label: "Table-level Governance", url: "https://www.linkedin.com/pulse/brief-guide-governance-apache-iceberg-tables-alex-merced-ckf4e", type: "blog" },
        { label: "Iceberg Table Governance", url: "https://atlan.com/know/iceberg/apache-iceberg-table-governance/", type: "blog" },
        { label: "Flink with Nessie", url: "https://www.dremio.com/blog/using-flink-with-apache-iceberg-and-nessie/", type: "blog" },
        { label: "REST Catalog Security", url: "https://medium.com/data-engineering-with-dremio/iceberg-rest-catalog-overview-3-oauth-authentication-34dc06a6aa20", type: "blog" }
      ]
    }
  }
];

export const flinkTableData = {
  title: "Apache Flink Iceberg Feature Matrix",
  description: "Comprehensive breakdown of Iceberg capabilities in Apache Flink 1.18+",
  variant: "default",
  columns: [
    {
      key: "dimension",
      header: "Dimension",
      tooltip: "Iceberg feature category or capability area",
      width: "w-64"
    },
    {
      key: "support",
      header: "Support Level",
      tooltip: "Level of support in Apache Flink",
      align: "center",
      width: "w-32"
    },
    {
      key: "details",
      header: "Implementation Details",
      tooltip: "Specific capabilities and limitations"
    },
    {
      key: "version",
      header: "Min Version",
      tooltip: "Minimum Flink version required",
      align: "center",
      width: "w-24"
    }
  ],
  rows: [
    {
      dimension: { 
        value: <span className="font-medium">Catalog Types</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "Complete", variant: "success" },
        tooltip: "Support for all major catalog implementations"
      },
      details: { 
        value: "Hive, Hadoop, REST (incl. Nessie), AWS Glue, JDBC, custom implementations",
        tooltip: "CREATE CATALOG statements with flexible catalog-type and catalog-impl options"
      },
      version: { value: "1.18+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">Batch & Streaming Reads</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "Complete", variant: "success" }
      },
      details: { 
        value: "Snapshot reads, incremental DataStreams, FLIP-27 source, exactly-once semantics",
        tooltip: "Unified API for both batch and streaming with configurable starting strategies"
      },
      version: { value: "1.18+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">Streaming Writes</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "Exactly-Once", variant: "success" }
      },
      details: { 
        value: "Checkpoint-based commits, INSERT INTO, automatic snapshot creation",
        tooltip: "Iceberg Sink commits on each Flink checkpoint for exactly-once guarantees"
      },
      version: { value: "1.18+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">DML Operations</span> 
      },
      support: { 
        value: <span className="text-yellow-600 dark:text-yellow-400 font-semibold">Partial</span>,
        badge: { text: "UPSERT Only", variant: "warning" }
      },
      details: { 
        value: "INSERT always available; UPSERT via write.upsert.enabled; no MERGE INTO in SQL",
        tooltip: "Row-level operations require v2 tables and upsert mode configuration"
      },
      version: { value: "1.18+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">CDC Integration</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "Reference Engine", variant: "success" }
      },
      details: { 
        value: "Native Debezium/Kafka CDC, Flink CDC connectors, pipeline connectors",
        tooltip: "Industry-leading CDC capabilities for database-to-lakehouse pipelines"
      },
      version: { value: "1.18+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">Format V3 Support</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "GA", variant: "success" }
      },
      details: { 
        value: "Deletion vectors, row lineage, new types (Flink 1.18+ + Iceberg 1.8+)",
        tooltip: "Comprehensive V3 support with some limitations in FLIP-27 source"
      },
      version: { value: "1.18+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">Time Travel</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "Source Options", variant: "success" }
      },
      details: { 
        value: "start-snapshot-id, start-snapshot-timestamp, branch, tag options",
        tooltip: "Comprehensive time travel via source configuration options"
      },
      version: { value: "1.18+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">Schema Evolution</span> 
      },
      support: { 
        value: <span className="text-yellow-600 dark:text-yellow-400 font-semibold">Limited</span>,
        badge: { text: "DDL Restrictions", variant: "warning" }
      },
      details: { 
        value: "ALTER TABLE properties only; no ADD/RENAME columns via SQL",
        tooltip: "Schema changes require external tools due to Flink DDL limitations"
      },
      version: { value: "1.18+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">Table Maintenance</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "Actions API", variant: "success" }
      },
      details: { 
        value: "Actions.rewriteDataFiles(), expire snapshots, remove orphans as batch jobs",
        tooltip: "Comprehensive maintenance operations via Actions API"
      },
      version: { value: "1.18+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">Security & Governance</span> 
      },
      support: { 
        value: <span className="text-green-600 dark:text-green-400 font-semibold">Full</span>,
        badge: { text: "Catalog Delegated", variant: "success" }
      },
      details: { 
        value: "Inherits catalog ACLs (Ranger, IAM, Nessie); REST auth with tokens",
        tooltip: "Enterprise security through catalog integration and modern auth"
      },
      version: { value: "1.18+" }
    },
    {
      dimension: { 
        value: <span className="font-medium">DDL Limitations</span> 
      },
      support: { 
        value: <span className="text-red-600 dark:text-red-400 font-semibold">Limited</span>,
        badge: { text: "Known Issues", variant: "error" }
      },
      details: { 
        value: "No computed columns, watermarks, or column ADD/RENAME in Iceberg DDL",
        tooltip: "Flink DDL has several limitations requiring external schema management"
      },
      version: { value: "N/A" }
    },
    {
      dimension: { 
        value: <span className="font-medium">SQL MERGE Operations</span> 
      },
      support: { 
        value: <span className="text-red-600 dark:text-red-400 font-semibold">None</span>,
        badge: { text: "Not Supported", variant: "error" }
      },
      details: { 
        value: "MERGE INTO not available in Flink SQL; use UPSERT mode instead",
        tooltip: "Requires programmatic UPSERT approach for merge-like operations"
      },
      version: { value: "N/A" }
    }
  ]
};

export const flinkUseCases = [
  {
    title: "Real-time CDC Pipelines",
    description: "Industry-leading change data capture from databases to data lakes",
    scenarios: [
      "Database-to-lakehouse replication with exactly-once semantics",
      "Multi-source CDC aggregation and transformation",
      "Real-time data synchronization across systems",
      "Event-driven architecture with streaming updates"
    ]
  },
  {
    title: "Stream Processing & Analytics",
    description: "Complex event processing with stateful computations",
    scenarios: [
      "Real-time fraud detection and alerting",
      "IoT sensor data processing and aggregation",
      "Financial trading and risk analytics",
      "Social media and clickstream analytics"
    ]
  },
  {
    title: "Data Lake Ingestion",
    description: "High-throughput data ingestion with automatic optimization",
    scenarios: [
      "Kafka-to-Iceberg streaming pipelines",
      "Multi-format data ingestion and standardization",
      "Schema evolution handling in streaming contexts",
      "Automatic data quality validation and cleansing"
    ]
  },
  {
    title: "Incremental ETL Processing",
    description: "Efficient incremental processing with checkpoint recovery",
    scenarios: [
      "Large-scale incremental transformations",
      "Historical data reprocessing with time travel",
      "Complex multi-stage pipeline orchestration",
      "Fault-tolerant processing with exactly-once guarantees"
    ]
  }
];

<QueryEngineLayout
  title="Apache Flink 1.18+"
  description="The reference implementation for CDC to Iceberg with comprehensive streaming support, exactly-once semantics, and advanced incremental reads"
  features={flinkFeatures}
  tableData={flinkTableData}
  useCases={flinkUseCases}
  officialDocs="https://flink.apache.org/docs/stable/"
  gettingStarted="https://iceberg.apache.org/docs/latest/flink-getting-started/"
  additionalResources={[
    { label: "Flink Configuration", url: "https://iceberg.apache.org/docs/latest/flink-configuration/" },
    { label: "Flink CDC Documentation", url: "https://nightlies.apache.org/flink/flink-cdc-docs-master/" },
    { label: "Flink Actions API", url: "https://iceberg.apache.org/docs/latest/flink-actions/" },
    { label: "Streaming Best Practices", url: "https://nightlies.apache.org/flink/flink-docs-stable/docs/dev/datastream/fault-tolerance/checkpointing/" }
  ]}
/>